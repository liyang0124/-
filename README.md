## 注意力机制

### 1)空间域注意力方法

#### 1.1. 自注意力

自注意力的结构下图所示，它是从NLP中借鉴过来的思想，因此仍然保留了Query, Key和Value等名称。对应图中自上而下分的三个分支，计算时通常分为三步：

（1） 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；

（2）第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；

（3）第三步将权重和相应的键值value进行**加权求和**得到最后的attention。

![v2-470ee9b60602fb151b6d4abf63b49772_r](C:\Users\ly\Desktop\李阳\v2-470ee9b60602fb151b6d4abf63b49772_r.jpg)

自注意力是基于特征图本身的关注而提取的注意力。对于卷积而言，卷积核的设置限制了感受野的大小，导致网络往往需要多层的堆叠才能关注到整个特征图。而自注意的优势就是它的关注是全局的，它能通过简单的查询与赋值就能获取到特征图的全局空间信息。

#### 1.2.非局部注意力

![img](https://pic1.zhimg.com/80/v2-5f7b1c1f7ebfb5ac588260f6316f4e0c_720w.jpg)

（1）首先对输入的feature map进行线性映射（1* 1聚氨基），然后得到θ，Φ，g特征。

（2）通过reshape来合并除了通道其他的维度，然后θ，Φ矩阵点乘，得到一个矩阵（这个是获取每帧每个像素对其他所有帧所有像素的关系）

（3）然后对刚刚的自相关特征进行softmax操作，得到0-1的权重（这就是selfattention系数）。

（4）后将attention系数，对应乘回特征矩阵g 中，然后再与原输入的特征图残差一下，获得non-local block的输出。

### 2)通道域注意力方法

#### 2.1.SENet

![image-20220413124832060](C:\Users\ly\AppData\Roaming\Typora\typora-user-images\image-20220413124832060.png)

分三个部分：

（a）squeeze：空间维度上进行特征压缩，将h×w×c转换为1×1×c的特征。它表示在特征通道上响应的全域性分布。算法很简单，就是一个全局平均池化。

（b）excitation:引入 w 参数来为每个特征通道生成权重，其中 w 就是一个多层感知器，是可学习的，中间经过一个降维，减少参数量。并通过一个 Sigmoid 函数获得 0~1 之间归一化的权重，完成显式地建模特征通道间的相关性。

（c）scale :将excitation的输出的权重看做是经过选择后的每个特征通道的重要性，通过通道宽度相乘加权到先前的特征上，完成在通道维度上的对原始特征的重标定

#### 2.2.SKNet

![image-20220413133509957](C:\Users\ly\AppData\Roaming\Typora\typora-user-images\image-20220413133509957.png)

主要创新点是设置了一组动态选择的卷积，分为三个部分操作Split、Fuse、Select。

（1）Split：对输入向量X进行不同卷积核大小的完整卷积操作（组卷积），特别地，为了进一步提升效率，将5x5的传统卷积替代为dilation=2，卷积核为3x3的空洞卷积；

（2）Fuse：类似SE模块的处理，两个feature map相加后，进行全局平均池化操作，全连接先降维再升维的为两层全连接层，输出的两个注意力系数向量a和b，其中a+b=1；

（3）Select：Select操作对应于SE模块中的Scale。Select使用a和b两个权重矩阵对之前的两个feature map进行加权操作，它们之间有一个类似于特征挑选的操作。

### 3）混合域注意力方法

#### 3.1.CBAM

#### 3.2 DANet

CBAM 和Non-local 的融合变形。具体来说就是，结构框架使用的是CBAM，具体方法使用的是self-attention。

![image-20220413133950852](C:\Users\ly\AppData\Roaming\Typora\typora-user-images\image-20220413133950852.png)
